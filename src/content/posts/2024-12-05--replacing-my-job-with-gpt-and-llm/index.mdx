---
hidden: true
unlisted: true
title: "Replacing Myself with AI"
subTitle: "How I learned to stop worrying and love the GPT."
label: "AI"
social_image: mobile.webp
category: AI
subCategory: Machine Learning
date: 2024-12-04
modified: 2024-12-05
tags: [ai, gpt, llm, machine-learning, nlp, automation]
cover_full_width: robot-dude-wide.webp
cover_mobile: robot-dude-square-200.webp
cover_icon: robot-dude-square-200.webp
---

In recent months, I've found myself increasingly relying on AI tools to handle repetitive tasks in my workflow. What started as a project to find out how well AI tools could create educational content has evolved into a reassuring reminder that humans are still in the early days of getting rocks to think, never mind guess-next-word-goodâ„¢ï¸ technology.

## The Challenge of Multiple Choice Assessments

One of my favorite teaching tools is **the unassuming multiple-choice question!**

### Pros of Multiple Choice Questions

1. Once written, they can be reused.
1. One of the cheapest "real" data points (for live events/talks/lectures.)
1. Time cost/attendee in-meetings is relatively fixed. 2-4 questions can be administered in 1-5 minutes.
1. Useful for self-guided/self-assessment as well. You can feel the skillz when you speed run 20 questions.
1. They can actually assess _some_ higher-order thinking skills like application and analysis.
1. Automatically graded.
1. Can be used to assess a wide range of skills and knowledge levels.

### Cons of Multiple Choice Questions

1. Most people have had negative MC test experiences. (Same!)
1. Easy to write bad & boring questions.
1. They are difficult to write well.
1. They can be gamed in unhelpful ways.
1. Zero cheating resistance. (Screenshots, etc.)

Some of these are deal-breakers for some folks, but I've found that MC questions can be a rapid & effective measurement tool in many contexts.

## Before We AI ðŸš€

Before we can dive into using AI, we need to understand what makes a good multiple-choice question.

## The Anatomy of a Good Multiple Choice Question

1. Don't get fancy with the question itself. Limit time to 10%.
1. **Craft `super distractors`â„¢ï¸** to exercise higher-order skills beyond knowledge recall.
1. Don't overlook your explanation. For deep & tailored questions, crafting a tailored explanation is key to locking in learning.

### The Question


{/* 
AI is amazing at coming up with initial ideas here.

Even some of my earliest prompts yielded encouraging results.

```markdown
> Generate 5 challenging multiple-choice questions about `Promise`, `async`/`await`, `Observable`, and `Generators`.
```

https://chatgpt.com/share/675283b2-5474-8000-8288-df44d4467b2d


```markdown
> Generate 5 very difficult multiple-choice questions about Rust Memory Handling. Include questions about `Borrow Checker`, `Ownership`, and `Lifetimes`, `Rc`, and `Arc`.
```

https://chatgpt.com/share/67527886-8db4-8000-a43d-892ad5777eb3 */}


### The Options

According to the popular Bloom's Taxonomy, brain engagement can be ranked on a scale:

1. The highest level of brain engagement involves **creation**.
1. The next highest is **evaluation**.
1. The third highest is **analysis**.
1. The fourth highest is **application**.
1. The fifth highest is **understanding**.
1. The lowest level is **remembering**.

Most MC Questions barely scratch the surface of Bloom's Taxonomy. They're often at the **remembering** or **understanding** level.

Many educators believe MCQs have limited value because they don't engage higher-order thinking skills.
But that's not a problem with the format itself - it's a problem with the questions.

Sure, it's easier to get higher confidence from a bit of written analysis or comparison. But it's not impossible to craft MCQs that engage higher-order thinking skills.


{/* The key to a good multiple-choice question is the **distractors** - the incorrect answers. They should be plausible, but not too plausible. */}



### The Explanation

This is a crucial moment to **lock in some learning!** To do that you need to **meet the learner** where they are, **explain the answer**, describe why it might be tricky, and **provide hints** for the future.


{/* **The question should be easy to understand.** Remember, there's no rule against including a clues - use bold, italics, or code backticks to highlight key terms. */}


Generally speaking, my quiz questions are a three-part package:

1. **Question**: A brief scenario or problem statement. **Easy**
2. **Options**: 4-10 possible answers, one of which is correct. **Tricky**
3. **Explanation**: A detailed explanation and/or hints. `Lock in the learning!` **Critical!**




**good question.** One that features quality `incorrect options` (known as _distractors_ in hip teacher lingo) are especially difficult. A well-crafted question can quickly test attention to detail, ability to adapt & apply knowledge, and even trip up the occasional expert.

It's pretty easy to spot a â€œbadâ€ question. Often they reveal lack of familiarity with best practices, real world gotchas, recent changes, debated nuances, etc.

I've worked with exceptional (human) curriculum writers who struggled with multiple choice content.

> So how can we get the most out of AI tools in this context?

## Research and Experimentation




## The Current State of AI-Assisted Question Writing

By Summer 2024, Anthropic's Claude showed promise in generating decent questions. Thatâ€™s some of the battleâ€¦ However, it still needed extensive guidance: specific code fragments, concepts, highlighted misconceptions - sometimes requiring so much detail that I it felt like I was practically re-writing the content 3x over myself.

While I typically delete (or re-write) 95% of AI-generated content, it's not all waste. I can get 1-2 extra questions per quiz thanks to ideas generated by LLMs - that's a 15% output boost!

The main issues? AI-generated content often suffers from overly formal or hokey tone, bad or obvious distractors, and occasionally incorrect or self-contradicting explanations.

## The Human Touch: What AI Still Can't Do

Experience-based insights, like knowing that shell environments handle integer arithmetic but not fractions (try dividing 1/2 versus 4/2), are still uniquely human territory.

AI particularly struggles with two crucial aspects:

- The strategic interplay between question order and distractor patterns
- Understanding human psychology in question design:
    - The power of super-close-but-not-quite options
    - Leveraging familiar but rarely-used concepts
    - Strategic use of adjacent technology concepts to create challenging distractors
    - Crafting questions that invite multiple solving strategies

## The Art of Distraction

Creating effective distractors (wrong answers) requires deep understanding of how humans think and learn. For instance, when testing database concepts, you might include terms like `CONCAT`, `COMBINE`, or `PIPE` alongside `JOIN` and `UNION` - i.e. terms that are related, or overlap with concepts from other databases and languages.

## Conclusion

While AI tools have become valuable assistants in content creation, they're far from replacing human expertise. The nuanced understanding of learning psychology, technical concepts, and question design strategy remains firmly in human territory - at least for now.

While the first [15+ Quizzes](/challenges) I published have been 99% free of AI/ML generated content, I'm hoping to continue to explore the potential of AI in educational content creation.

## What's Next?

Stay tuned! I'll be posting more on my journey developing high quality + rigorous educational material - with a bit of help from our AI overlords! ðŸ¤–ðŸš€
