---
title: "From Zero to Regex Hero"
subTitle: "Extract & Parse URL-like Strings with Regex"
social_image: desktop-social.webp
category: Regex
subCategory: Data Extraction
date: 2024-12-28
modified: 2024-12-29
tags: [regex, url, data-extraction, data-processing]
cover_full_width: regex-url-parsing-wide.webp
cover_mobile: regex-url-parsing-square-200.webp
cover_icon: regex-url-parsing-square-200.webp
---

Extracting URL-like strings from raw text can feel like a never-ending game of whack-a-mole: punctuation, parenthetical wrappers, and ambiguous formatting all conspire to break your best-laid regular expressions. In this post, we'll tackle the problem head-on with a flexible, two-step approach. Our goal is to **vacuum up** suspicious URL-ish text first, and then handle validation in a subsequent pipeline.

<p class="breakout">Do not use this pattern for <b>validating</b> URLs; it's meant for sloppy data <b>extraction</b>.</p>

*(Original inspiration and discussion can be found here: [StackOverflow Answer](https://stackoverflow.com/a/34669019/369727))*

TL;DR: Check out [a live demo of the regex](https://regex101.com/r/jO8bC4/62) in action.

---

## 1. Overview: Two Steps to URL Extraction

1. **Hoover Up Everything**
   Cast a wide net to grab all strings that *could* be URLs. This is where our "intimidating" regex flexes its muscles.

2. **Validate**
   Once you've captured these candidates, you can pass them through a rigorous check (DNS resolution, comparison against known domains, etc.) to weed out duds. That part will be left for the pipeline in step two.

**Disclaimer:** This is not meant to be perfect regex. This is work-horse Regex. It has a job to do: Extract & parse anything URL-shaped from raw text.

## 2. The "Intimidating" Regex

Below is a one-line regex designed to catch a variety of protocols, domains, paths, and optional query/fragment sections:

Don't worry if it looks like a jumbled mess at first glance.

<p class="breakout">We're going to break it down step-by-step in a moment.</p>

**Note:** If the pattern gets cut off, don't worry; we'll [break it down in the next section.](#how-it-works-step-by-step)

```js frame="code" title="URL-like String Extractor"
const re = /([a-z0-9-]+\:\/+)([^\/\s]+)([a-z0-9\-@\^=%&;\/~+]*)[\?]?([^ \#\r\n]*)#?([^ \#\r\n]*)/gi
// (Compatibility: ES2015+)
// Length: 84 characters
```

For completeness, here's a more robust version that handles some edge cases better:

```js frame="code" title="More Advanced ES2018 Version"
const re = /(\b[\.\w-]+:\/{2,3})(?!\.)(?!.*\.{2})(?!-)(?!.*-\.)(?!.*@\.)([\w\-@\^=%&:;~\+\.]+(?<!-))(\/+[\w\-@\^=%&:;\/~\+\.]+(?<!\.))?\??([\w=\-&@$!|~+_]+)*#?([\w=\-&@$!|~+_]+)*/gi
// (Compatibility: ES2018+, Node.js 12+)
// Better handling of edge cases
// Length: 166 characters
```

We're going to break down the simpler (1st version) for now:

### How It Works, Step-by-Step



When performing data extraction (as opposed to strict validation), I often combine **negated character classes** `[^a-z]` with either a `+` or `*`.

You might recognize this trick from matching HTML/XML tags. The pattern `<[^>]*>` captures anything between `<` and `>`, while `<[^>]+>` captures at least one character between the brackets. It relies on the fact that we can match "anything but a closing angle bracket" to grab an entire tag. (Side note: there are safer methods for parsing HTML/XML. Avoid RegEx when there's safer native interfaces available. Trust me, hackers love sneaking `>` in the middle of another tag to see if your validation missed the script tag they injected.)

<p class="breakout inset">To my CS friends: Calm your tokenizers! Before you dequeue your stack! I know there's several solutions in this space. Some even "proper" ones. Out of scope, for now.</p>

Now, consider parts of a URL: A query string, follows a similar pattern where `?` is followed by one-or-more **non-`?`** characters. Similarly, the fragment identifier begins with a `#` followed by **non-`#`** characters. (There's a bit more to it, since we capture the hash using `[^ \#\r\n]*`, which matches everything until encountering spaces, `#`, or newlines.)


```text
1) Capture `protocol`: `([a-z0-9-]+\:\/+)`
2) Capture `domain`: `([^\/\s]+)`
3) Capture `path`: `([a-z0-9\-@\^=%&;\/~\+]*)`
4) Optionally capture `query`: `([\?]?([^ \#\r\n]*))`
5) Optionally capture `fragment`: `(#?([^ \#\r\n]*))`
```

- `Protocol` **Group 1** `([a-z0-9-]+\:\/+)`
  1. Looks for an alpha-numeric-dasherized protocol (e.g., `http://`, `https://`, `ftp://`, `mailto://`) by matching `[a-z0-9-]+`
  2. The `\:\/+` part ensures we have one or more slashes after the colon, to capture possible typos. To make this more strict, you could replace `+` with `{2,3}` to allow two OR three slashes. This change would catch `file:///` but not `file:/`.

- `Host` **Group 2** `([^\/\s]+)`
  Captures the domain (e.g., `example.com`, `localhost:8080`.) Again, we use a similar strategy, since it's easier to list what is not permitted in a domain. In this case, slashes `\/` and whitespace `\s`. Then we use `+` to capture at least one (or more) characters.  

- `Path` **Group 3** `([a-z0-9\-@\^=%&;\/~\+]*)`
  Just when you are getting used to inverted logical matching, I throw a regular character class expression at you!   Matches path or parameter characters, allowing letters, numbers, common URL-safe symbolsâ€”enough to catch all sorts of oddities.

- `Query` **Group 4** `[\?]?([^ \#\r\n]*)`
  The bit at the beginning, `[\?]?`, matches zero-or-one `?` - marking the start of a querystring. In my use case, I simplified my secondary processing by excluding the `?` symbol itself from the match groups. The rest of the query string is captured by `([^ \#\r\n]*)`, which matches anything that's not a space, `#`, or newline.

- `Hash` **Group 5** `#?([^ \#\r\n]*)`
  Captures any fragment identifier starting with `#`. Similar to the query string, it matches anything that's not a space, `#`, or newline.

**NOTE:** This pattern is **NOT meant to be an RFC-compliant** solutionâ€”it's deliberately broad to sweep up all plausible URL-like strings. Once captured, you can sort out the real URLs from the duds in post processing - based on your needs.

## 3. Parsing Example

Here's a short JavaScript example that uses this regex to extract & (optionally) parse URL-like strings from a given input:

```js frame="code" title="js-example.js"
const text = `
Check this out: https://example.com/path?query=123#section
And also (ftp://files.server.org/index).
Plus a weird one: custom-scheme://host/param;weird^stuff
`;

const urlRegex = /([a-z0-9-]+\:\/+)([^\/\s]+)([a-z0-9\-@\^=%&;\/~\+]*)[\?]?([^ \#\r\n]*)#?([^ \#\r\n]*)/gi;

// ðŸš¨ IMPORTANT: `match[0]` returns the entire match.
//    Use `match.slice(1)` to get the parts.
const matches = [...text.matchAll(urlRegex)]
  .map(match => match[0]);
// Extracting parts of the URL
const parts = [...text.matchAll(urlRegex)]
  .map(match => match.slice(1));

console.log(matches, parts);
```


### Extracted (Whole) URLs


```json
[
  "https://example.com/path?query=123#section",
  "ftp://files.server.org/index).",
  "custom-scheme://host/param;weird^stuff"
]
```

### Extracted Parts

```json
[
  [
    "https://",     // Protocol
    "example.com",  // Domain
    "/path",        // Path
    "query=123",    // Query
    "section"       // Hash
  ],
  [
    "ftp://",           // Protocol
    "files.server.org", // Domain
    "/index",           // Path
    ").",               // Query
    ""                  // Hash
  ],
  [
    "custom-scheme://",   // Protocol
    "host",               // Domain
    "/param;weird^stuff", // Path
    "",                   // Query
    ""                    // Hash
  ]
]
```


## Next Steps

Depending on your use case, you might need to refine this regex or add more validation & post-processing steps.

### Different Projects, Different Needs

Different projects have different needs & security concerns:

1. **Web Scraping**
  You might need to validate URLs to ensure they're reachable or to filter out known malicious domains.
2. **Data Processing**
  If you're extracting URLs from user-generated content, you might want to ensure they're safe to visit.
3. **Data Analysis**
  If you're analyzing URLs for research or marketing purposes, you might need to filter out duplicates or irrelevant links.
4. **User-facing Chat**
  If you're building a chat app, you might want to automatically hyperlink URLs for users.

### Post-Processing & Validation

Once you've gathered everything that looks remotely like a URL, you can run them through more rigorous checks, as needed:

- **DNS Lookup**
  Verify the domain actually resolves.
- **Similarity Scoring**
  Compare new links against existing ones to detect duplicates or very close variants.
- **Custom Rules**
  Restrict TLDs, reject known malicious patterns, limit length, or anything else needed for your use case.

## Final Thoughts

- **Keep it simple for extraction;** make your life easier by casting a wide net, then refine afterward.
- **Regex is powerful but not omnipotent.** Don't expect it to be a perfect judge of URL correctness.
- **Iterate and test.** As you encounter new real-world texts, refine your regex or your validation pipeline to address emergent edge cases.

By following these steps, you can quickly and effectively extract URL-like strings from raw text, setting the stage for further processing and validation.

### References

- [StackOverflow Answer (Original Inspiration)](https://stackoverflow.com/a/34669019/369727)
- [MDN Docs on Regular Expressions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions)
- [RFC 3986 - URI Generic Syntax](https://datatracker.ietf.org/doc/html/rfc3986)
