---
title: "Who put vulns in my patch?"
subTitle: "When updates can't save you"
date: 2025-09-10
modified: 2025-09-14
tags: [security, patches, software-updates, risk-management, enterprise-it]
category: Security
subCategory: Vulnerability Management

cover_full_width: ./neeqolah-creative-works-u8Kyb3ZV_WI-unsplash_wide.webp
cover_mobile: ./neeqolah-creative-works-u8Kyb3ZV_WI-unsplash_square_300.webp
cover_icon: ./neeqolah-creative-works-u8Kyb3ZV_WI-unsplash_square_300.webp
cover_credit: Photo by <a href="https://unsplash.com/@neeqolah?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Neeqolah Creative Works</a> on <a href="https://unsplash.com/photos/a-multicolored-sculpture-of-a-fish-on-a-white-background-u8Kyb3ZV_WI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
---

## Welcome to the Security Theater

Every security patch is a love letter to attackers. While IT teams scramble to test and deploy fixes across their infrastructure, threat actors are already reverse-engineering the patches, extracting vulnerability details, and weaponizing exploits. The asymmetry is brutal: vendors take weeks to develop fixes, enterprises need days to roll them out, but attackers can diff binaries and scan for vulnerable systems in hours.

<blockquote class="inset">
Patches mitigate yesterday's exploits & introduce tomorrow's surprises. âœ¨
</blockquote>

_Welcome to Patch Tuesday, otherwise known as Exploit Wednesday Eve._

## The Best Intentions

Even without savvy attackers lurking, patches alone can be a problem. The CrowdStrike incident of July 2024 exposed an uncomfortable truth: even when you follow best practices, patches are unknown software. They can destroy what they're meant to protect. One bad update turned airports into disaster zones and hospitals into paper-chart museums. Yet if you skip the wrong patch, it could cost more than most companies' annual revenue. You're trapped between installing updates that might break everything and leaving holes that definitely will.

## The Lies We Tell Ourselves

Sadly, it is possible to spend all the money in the world on security and still get breached. Perhaps ironically, spending all the money makes breaches _more likely_: the more overlapping layers you add, the more burdensome & complex your security measures become, the harder they are to support, integrate, maintain and monitor.

How can we know the "right" level of investment? The "best" countermeasures and practices? What is the optimal balance between security and usability?

The answer is easy: It depends, both on you & your organization!

Don't roll your eyes too hard. At least the criteria is _you & your org_. Hopefully you can find a Subject Matter Expert (SME.)

## Quitting Security Theater Camp

Get serious. Seek to continually know thy changing risks.

Write things down: Understand your environment, what scenarios should _your_ organization document as threats? How often should you test your incident response plan? Or your data recovery plan? What is your tolerance for downtime, data loss, or reputational damage? If data is stolen or lost, what legal obligations do you have to your customers, partners, or shareholders? Do employees know their roles in these scenarios?

### Critical Components

> Are there any (mostly) universal best practices? Yes, a few:

- Invest in YubiKeys, for EVERYONE. Or at least mandate passkeys.
  - You can social engineer a One-time code. You cannot social engineer a hardware token like that.
  - Enforce MFA on every service, everywhere.
- Backups.
  - All cloud infra must have "Offline" backups. Preferably immutable, and geographically distributed.
    - Offline Cloud backups? Meaning, across account or vendor. E.g. if you're on AWS, you should have backups in GCP, Azure, and vice versa. Or consider 3rd party S3 solutions, like Backblaze B2.
  - Schedule quarterly test "recovery drills" (restore entire cloud infra in an unused region using only backups, snapshots, and maybe some light terraform/CFN/CDK.).
  - Don't forget employee laptops, who may need an alternate plan for when they're struggling to connect to a convention center's Wi-Fi with 20K other attendees. (This is more into SLA, recovery time objectives, etc.)

You need to determine what your risk profile includes. Does your app access bank/crypto accounts? Do you even have a web app? (XSS, CSRF) Are your engineers aware of supply chain attacks? Do you know how insider threats work? How many public services do you expose to the world? (i.e. your zero-day exposure) What is your tolerance for data loss, ransomware, reputational damage, or regulatory fines? Can you afford any downtime? Maybe, is your backup/revert process faster than a full re-deployment? Weigh the possibility of a breach versus the costs of mitigations, and the risk of a failed upgrade. It is different for every organization.

## The Other Side of Fear

{/* Trace paths through your dependencies, infra, inputs/outputs, etc. */}

The solution isn't sexy or singular. It is multifaceted: defense in depth, rotating offline backups, disaster drills, compensating controls, and accepting that patches are necessary evils, not magic spells. Apply with caution, even if automatically, use labels to stage & sequence updates, test thoroughly, and if possible, anticipate failures with one or two fallback plans. At least when it matters.


