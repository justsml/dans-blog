---
title: "Foreign Keys: Stop Asking If They're Fast"
subTitle: "Ask what you're actually optimizing for."
date: 2025-12-29
modified: 2026-01-10
tags: [postgres, postgresql, databases, performance, foreign-keys, constraints, indexing]
category: Code
subCategory: Databases
cover_full_width: ./wide.webp
cover_mobile: ./square.webp
cover_icon: ./square.webp
---

A startup CTO once told me they removed all Foreign Keys to "optimize for web scale."

Six months later: 2b+ orphaned records. Billing system charging deleted users. Analytics off by 40%.

"But Foreign Keys are slow!" they protested.

Sure. And safety glass, seatbelts, and airbags add weight to your car. Should you remove those too?

---

## The Wrong Question

Everyone asks: "Are Foreign Keys fast or slow?"

Nobody asks: "What am I actually trying to prevent?"

If you're building a billing system, orphaned invoices pointing to deleted users is a lawsuit waiting to happen. Pay the FK tax.

If you're logging weather sensor data at 10,000 readings per minute on a Raspberry Pi, maybe you don't need to validate that `sensor_id` exists in the `sensors` table on every single insert. The sensor exists. You just talked to it.

The performance cost is real. But it's not the point.

---

## When Constraints Actually Matter

Let's get specific. You're building a weather monitoring system.

You have tables: `weather_stations`, `sensor_devices`, `sensor_readings`, `us_states`.

Do you FK everything together?

Think about what changes and what doesn't:

- US States? Those aren't changing. Wyoming isn't getting renamed. You don't need a FK to validate state codes on every insert.
- Weather stations? These get added, moved, decommissioned. But do you want historical readings to "lose" their station if someone fat-fingers a deletion? Maybe you want the data to stay intact even if the station record is gone.
- Sensor readings? You're inserting thousands per minute. Every FK check is a lookup. Every lookup is contention on your tables.

The question isn't "should I use FKs?"

The question is: "What breaks if this reference is wrong, and what breaks if validation is slow?"

If wrong references mean corrupted billing, use FKs.

If slow validation means your insert queue backs up and you lose real-time data, skip them.

---

## The Append-Only Escape Hatch

You need max speed inserts. Your queue is piling up. Transactions are timing out.

You have options:

1. Change your isolation level - `READ COMMITTED` is faster than `SERIALIZABLE`. But now you're trading consistency for speed.
2. Batch your commits - Insert 1000 rows per transaction instead of 1. Amortize the FK overhead.
3. Denormalize into an append-only log - No FKs. Just leave chunks of data together in JSONB and write it once.

That third option? It's not "wrong." It's a tradeoff.

```sql
CREATE TABLE sensor_log (
  id BIGSERIAL PRIMARY KEY,
  recorded_at TIMESTAMPTZ NOT NULL,
  data JSONB NOT NULL  -- { station_id, sensor_id, temp, humidity, ... }
);

CREATE INDEX ON sensor_log USING GIN (data);
CREATE INDEX ON sensor_log (recorded_at);
```

No joins. No FK checks. Just append. Query by time range or GIN index on the JSONB blob.

Is this "best practice"? No.

Does it work when you're inserting 50,000 rows per minute on a ~$35~ $100 Raspberry Pi? Yes.

Go ahead, FK it, or don't. I won't tell mom.

---

## The Normalization Trap

Database textbooks teach you to normalize everything. Avoid duplication. 3rd Normal Form or bust.

So you end up with this:

`Orders` → `OrderItems` → `Products` → `Variants` → `Colors` → `Sizes`

Six table joins to answer: "Did I order the red shirt or the blue one last X-Mas?"

"But what if the brand's name for Blue changes?"

If it does, do you really want historical orders to retroactively change color?

This is the thing nobody talks about: some data is a snapshot, not a reference.

When you place an order, you're buying "Blue T-Shirt, Size M" as it exists *right now*. Not as it might be defined in the catalog next year.

Denormalize at write time. Store `{"color": "blue", "size": "M"}` on the order. Done.

### How to Spot Snapshot Data

Ask yourself: "Is this a point-in-time record?"

- Orders - Product details at purchase time
- Audit logs - User state when they logged in  
- History tables - Record state before update
- Event streams - What happened, when, with what data

If yes, stop joining. Start snapshotting.

### Opaque Blobs (Not for Querying)

Some data isn't even a snapshot. It's just a whole logical object you store and retrieve.

- LLM model configs - `{"model": "gpt-4", "temperature": 0.7, "max_tokens": 2000}` - You're not querying by temperature. You're fetching the whole config by request ID.
- JWT payloads - The decoded token data. You store it. You retrieve it. You don't index it.
- API request/response logs - The full JSON blob. For debugging, not querying.
- User preferences - Theme settings, notification flags. Always fetched as a unit by user ID.

These don't need normalization. They don't need FKs. They're opaque blobs. Shove them in JSONB and move on.

The 6-table join to find out what color shirt was ordered? That's not "proper normalization." That's treating historical data like it's live data.

(See: [The JSONB Seduction](/the-jsonb-seduction) for when this backfires.)

---

## Scale Is Relative

Popular opinion: "Foreign Keys don't scale."

Reality: Scale is relative to your hardware.

A Raspberry Pi logging 10K sensor readings per minute? That's high scale for a microSD card. AWS Aurora with provisioned IOPS? You can FK your way through billions of rows without breaking a sweat.

The real limit isn't row count. It's sharding.

When `Users` live on Server A and `Orders` live on Server B, Foreign Keys physically cannot work. The database has no way to enforce a constraint across network boundaries.

You hit this when your IoT fleet has 50,000 edge devices each running SQLite, or your multi-tenant SaaS isolates each customer into their own database for compliance.

At that point, you're already running background jobs to find orphans and implementing eventual consistency. So why optimize for a problem you don't have yet?

---

## What to Actually Do

Stop asking "should I use Foreign Keys?"

Start asking:

1. What breaks if this reference is wrong? (Lawsuits? Or just a missing join?)
2. What breaks if validation is slow? (Lost data? Or just slower queries?)
3. Is this data a snapshot or a reference? (Historical record? Or live lookup?)

Then decide:

Use FKs for:
- Money (Invoices → Users)
- Auth (Sessions → Users)  
- Anything where corruption means lawsuits

Skip FKs for:
- High-volume logs (1M events/min don't need validation)
- Append-only time-series data
- Cross-shard references (physically impossible)

Denormalize for:
- Historical snapshots (orders, audit logs)
- Data you fetch as a blob (user preferences)
- Schemas you don't control (webhook payloads)

---

## Conclusion

Foreign Keys aren't a performance problem. They're a tradeoff.

You're trading write speed for data integrity. Sometimes that's worth it. Sometimes it's not.

But if you're removing FKs because you read a blog post about "web scale," you're cargo-culting Netflix's problems onto your 10-user SaaS.

Know your constraints. Know your goals. Then optimize for what actually matters.
