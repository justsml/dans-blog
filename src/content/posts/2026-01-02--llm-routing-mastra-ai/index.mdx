---
title: "Don't Marry Your Model: The Case for LLM Routing"
subTitle: "Why sticking to one provider is costing you money and quality."
date: 2026-01-02
modified: 2026-01-08
tags: [AI, LLM, TypeScript, Mastra, Agent Orchestration]
category: AI
subCategory: Engineering
cover_full_width: ./wide.webp
cover_mobile: ./square.webp
cover_icon: ./square.webp
---

> [!NOTE]
> **Mastra v1 Beta**
>
> This article uses the Mastra v1 Beta.

I had a client recently who insisted on using the most expensive model for *everything*.

Simple sentiment analysis? **$30/million tokens.**
JSON formatting? **$30/million tokens.**
Calculating 2+2? **$30/million tokens.**

They were burning venture capital to heat the room.

**The truth is simple: No single model is the best at everything.**

*   **Claude Sonnet 4.5** is the coding king.
*   **Gemini 3 Pro** has the context window of a god.
*   **GPT-5.2** is the reasoning engine.
*   **Haiku/Flash** are for the "grunt work."

**LLM Routing** isn't fancy. It's just smart delegation.

---

## Enter the Orchestrator

Mastra makes this trivial. You create a "Router Agent"â€”a middle manager whose only job is to delegate tasks to the experts.

(Yes, middle managers are finally useful.)

### The Architecture

We are going to build a system with three specialists:

1.  **The Coder** (Claude)
2.  **The Creative** (Gemini)
3.  **The Generalist** (GPT)

And one **Boss** (GPT-Mini) to tell them what to do.

### The Code

First, hire your specialists.

```typescript
// ./src/mastra/index.ts
import { Mastra } from '@mastra/core';
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';

export const claudeAgent = new Agent({
  id: 'claude-agent',
  instructions: 'You are an expert engineer. Write bugs? You are fired.',
  model: anthropic('claude-sonnet-4.5'),
});

export const geminiAgent = new Agent({
  id: 'gemini-agent',
  instructions: 'You are a creative writer. Be weird.',
  model: google('gemini-3-pro'),
});

export const gptAgent = new Agent({
  id: 'gpt-agent',
  instructions: 'You are a helpful assistant. Be boring.',
  model: openai('gpt-5.2'),
});
```

Now, the Router.

```typescript
export const routerAgent = new Agent({
  id: 'router-agent',
  name: 'The Boss',
  instructions: `You are an intelligent router.
  - Coding -> Claude
  - Poetry -> Gemini
  - Facts -> GPT

  Do not do the work yourself. Delegate.`,
  model: openai('gpt-5-mini'), // Use a cheap model for routing!
  agents: {
    claudeAgent,
    geminiAgent,
    gptAgent,
  },
});

export const mastra = new Mastra({
  agents: { routerAgent, claudeAgent, geminiAgent, gptAgent },
});
```

Now, when you talk to the Router, it acts as a proxy.

*   Customer: "Write a bubble sort."
*   Router: "That looks like code. Hey **Claude**, handle this."
*   Claude: "Here is your python code..."
*   Router: "Here is the result."

### Why This Matters

1.  **Cost**: Using a small model to route requests saves a fortune. You only pay for intelligence when you need it.
2.  **Quality**: Claude is better at Code. Gemini is better at Creative. Why compromise?
3.  **Resilience**: If OpenAI goes down (again), your router can send traffic elsewhere.

Stop acting like a fanboy for one specific provider. The best engineers have a toolbox, not just a hammer.

### Resources

- [Mastra.ai Documentation](https://mastra.ai/docs)
- [Mastra GitHub Repository](https://github.com/mastra-ai/mastra)

## Read the Series

1. **LLM Routing** (This Post)
2. [Security & Guardrails](/mastra-security-guardrails)
3. [MCP & Tool Integrations](/mastra-mcp-tool-integrations)
4. [Workflows & Memory](/mastra-workflows-memory)
