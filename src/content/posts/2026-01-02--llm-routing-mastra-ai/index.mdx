---
title: "Competing Models: LLM Routing with Mastra.ai"
subTitle: "Dynamically select the best AI model for each task"
date: 2026-01-05
modified: 2026-01-06
tags: [AI, LLM, TypeScript, Mastra, Agent Orchestration]
category: AI
subCategory: Engineering
cover_full_width: ./wide.webp
cover_mobile: ./square.webp
cover_icon: ./square.webp
---

In the rapidly evolving landscape of AI, one truth has become clear: no single model is the best at everything. **GPT-5.2** is a ~reasoning powerhouse~ bigger version than GPT-5.1, **Claude Sonnet 4.5** dominates in coding tasks, and **Gemini 3 Pro** excels in long-context understanding.

As developers, we shouldn't have to choose just one. **LLM Routing** is the pattern of dynamically selecting the best model for a specific task. By routing user queries to the most capable model, we can build applications that are smarter, faster, and more cost-effective.

## Enter Mastra.ai

[Mastra.ai](https://mastra.ai) is a new TypeScript agent framework designed to make building complex AI systems simple. Unlike other frameworks that can be overly opinionated or heavy, Mastra provides just the right primitives for agents, workflows, and tools.

One of its most powerful features is how easily it enables **Agent Orchestration**. You can create a "Router Agent" whose sole job is to understand intent and delegate work to specialized sub-agents.

## The Demo: A Multi-Model Orchestrator

Reflecting the need for specialized intelligence, I've built a demo that routes queries to three specialized agents:

- **The Coder**: Claude Sonnet 4.5 (via [Anthropic](https://www.anthropic.com/))
- **The Creative**: Gemini 3 Pro (via [Google](https://deepmind.google/technologies/gemini/))
- **The Generalist**: GPT-5.2 (via [OpenAI](https://openai.com/))

### The Architecture

We use a top-level **Router Agent** powered by GPT-5-Mini. This agent doesn't answer questions directly. Instead, it has access to three custom agents: `claudeAgent`, `geminiAgent`, and `gptAgent`.

When a user asks "Write a bubble sort in Python," the Router Agent reasons: "This is a coding task. I should ask the Coder." It then delegates the interaction to `claudeAgent`.

### Code Walkthrough

Here is how simple it is to set up in Mastra.

**1. Configure the Sub-Agents**

First, we define our specialists. Note how we give them specific instructions:

```typescript
// ./src/mastra/index.ts
import { Mastra } from '@mastra/core';
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';

export const claudeAgent = new Agent({
  id: 'claude-agent',
  name: 'Claude Coder',
  instructions: 'You are an expert software engineer. Provide high-quality, bug-free code snippets.',
  model: anthropic('claude-sonnet-4.5'),
});

export const geminiAgent = new Agent({
  id: 'gemini-agent',
  name: 'Gemini Creative',
  instructions: 'You are a creative writer. Generate engaging and imaginative content.',
  model: google('gemini-3-pro'),
});

export const gptAgent = new Agent({
  id: 'gpt-agent',
  name: 'GPT-5.2 Generalist',
  instructions: 'You are a helpful generalist assistant. Answer questions directly and concisely.',
  model: openai('gpt-5.2'),
});
```

**2. The Router Agent**

The router is just another agent, but we configure it with `agents` so it can delegate tasks. This pattern is cleaner than defining custom tools for each interaction:

```typescript
// src/agents.ts continued
export const routerAgent = new Agent({
  id: 'router-agent',
  name: 'Orchestrator',
  instructions: `You are an intelligent router. Your job is to analyze the user's request and delegate it to the most appropriate specialist agent.
  - For coding tasks, use the "Claude Coder" agent.
  - For creative writing, use the "Gemini Creative" agent.
  - For general questions, use the "GPT-5.2 Generalist" agent.
  
  Do not answer the user directly unless you cannot find a suitable agent.
  Return the response from the agent you selected.`,
  model: openai('gpt-5-mini'),
  agents: {
    claudeAgent,
    geminiAgent,
    gptAgent,
  },
});

export const mastra = new Mastra({
  agents: { gptAgent, claudeAgent, geminiAgent, routerAgent },
});
```

**3. Running the Routing**

In our application, we just talk to the router:

```typescript
// src/index.ts
import { mastra } from './mastra';

async function main() {
  const router = mastra.getAgent('routerAgent');
  
  const queries = [
    "Write a bubble sort algorithm in Python.",
    "Write a short poem about a robot learning to love.",
    "What is the capital of France?"
  ];

  for (const query of queries) {
    console.log(`\nðŸ‘¤ User: ${query}`);
    const result = await router.generate(query);
    console.log(`ðŸ¤– Response: ${result.text}`);
  }
}

main().catch(console.error);
```

## Alternate Approach: Tool-Based Routing

While `agents` is the preferred way to delegate work to other LLMs, you can also use **Tools** for routing. This is useful when:

- You need to execute code or call external APIs (not just LLMs).
- You require strict structured output validation before delegation.
- You have a complex multi-step workflow that needs to be orchestrated via a state machine.

In that case, you would define `tools` on your agent, where each tool's `execute` function performs the necessary logic.

## Why This Matters

This pattern unlocks significant potential:

1. **Quality**: Use the SOTA model for each specific domain.
2. **Cost**: Route simple queries to cheaper, faster models (like Haiku, GPT-5-Mini or Gemini 3 Flash) and reserve heavy hitters for complex reasoning.
3. **Reliability**: If one provider goes down, your router can failover to another model.

Mastra.ai makes implementing this pattern type-safe and intuitive. It handles the complexity of different API providers (OpenAI, Anthropic, Google) so you can focus on the orchestration logic.

## What's Next

This is just the beginning. In upcoming posts, we'll explore:

- **Security & Guardrails** (Tomorrow): Content moderation, PII detection, and prompt injection defense
- **MCP & Tool Integrations** (Jan 4): Connect to Salesforce, HubSpot, and thousands of pre-built integrations
- **Workflows & Memory** (Jan 5): Build complex multi-step processes with persistent context

## Conclusion

LLM routing is moving from a nice-to-have to a necessity for production-grade AI applications. With frameworks like [Mastra](https://mastra.ai), building these intelligent systems is easier than ever.

The full code for this demo is available in the [mastra-routing-demo](https://github.com/justsml/dans-blog/tree/main/mastra-routing-demo) folder. Try it yourself and experience the power of intelligent model routing!

### Resources

- [Mastra.ai Documentation](https://mastra.ai/docs)
- [Mastra GitHub Repository](https://github.com/mastra-ai/mastra)
- [AI SDK by Vercel](https://sdk.vercel.ai/docs)
- [OpenAI Platform](https://platform.openai.com/)
- [Anthropic Claude](https://www.anthropic.com/claude)
- [Google Gemini](https://deepmind.google/technologies/gemini/)
