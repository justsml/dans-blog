---
title: "JSONB: The Best Way to Ruin Your Database"
subTitle: "How your 'temporary' schema-less column became permanent technical debt."
date: 2025-12-29
modified: 2025-12-30
tags: [postgres, postgresql, databases, jsonb, json, schema-design, technical-debt]
category: Code
subCategory: Databases
cover_full_width: ./wide.webp
cover_mobile: ./square.webp
cover_icon: ./square.webp
---

"We'll just put it in a JSONB column for now. We can normalize it later."

**Famous last words.**

I have heard this ten times. I have seen it fixed zero times.

That "temporary" column is now three years old. It contains 40 different versions of a user profile. It is queried by 15 microservices, each assuming a different schema.

**JSONB is Technical Debt,** deploy it wisely-or not at all.

---

## The Seduction

You are building a feature. You don't know if a User has a `twitter_handle` or a `bluesky_handle`.

Instead of doing the work (Schema Design), you do this:

```sql
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  profile JSONB -- "Flexible!"
);
```

It feels great. You ship feature A. Then feature B. Then feature C.

## The Hangover

### Month 6: The Query From Hell

Product asks: *"How many users are in New York?"*

You write:
```sql
SELECT count(*) FROM users WHERE profile->>'location' = 'New York';
```

**Postgres conducts a Full Table Scan.** Every single row.

So you add a GIN index. Now your write performance tanks because GIN indexes are huge and expensive to update.

### Year 1: Schema Drift

You have three versions of data in the same column.

*   Row 1: `{"city": "NYC"}`
*   Row 1000: `{"location": "NYC"}`
*   Row 5000: `{"address": {"city": "New York"}}`

Your application code now looks like this:

```javascript
const city = user.location || user.city || user.address?.city || "Unknown";
```

**You didn't remove the schema. You just moved the validation from the Database (where it belongs) to your Application (where it rots).**

---
---

## When to Actually Use JSONB

I am not a luddite. JSONB has valid use cases. **Many times it's perfectly fine—even optimal.**

The critical distinction: **Do you need to query ad-hoc key paths, or are you storing opaque blobs?**

### Legitimate JSONB Use Cases

1.  **Webhook Payloads**: You receive data from Stripe, Slack, or GitHub. You have zero control over the schema. You may never query it. You just need to store it for debugging or replay. **Perfect for JSONB.**

2.  **Logging & Event Streams**: Application logs, audit trails, error contexts. These are write-heavy, rarely queried by specific fields, and often analyzed in bulk or exported to analytics platforms. **JSONB is fine here.**

3.  **User Preferences & Settings**: Settings objects where you have 100+ boolean flags, most are false, and you're always fetching the entire blob by user ID. You're not running `WHERE preferences->>'theme' = 'dark'`. **JSONB works.**

4.  **API Response Caching**: You're caching entire API responses. The database is just a faster Redis. You fetch by cache key, never by nested properties. **JSONB is appropriate.**

5.  **Event Sourcing**: You're storing immutable event payloads. Your queries are always "give me all events for aggregate X" ordered by time. You never run WHERE clauses on event properties. **JSONB fits.**

**Rule of Thumb:** If you put it in a `WHERE` clause with dynamic paths, it should probably be a Column. If it's always fetched as a complete blob by a known key, JSONB is fine.

### At Scale: Object Versioning > Normalization

Here's where it gets interesting. At sufficiently large scale, the "right" solution isn't normalization—it's **object versioning.**

If you have billions of rows and frequent schema evolution, migrating columns becomes expensive. Companies like Stripe, GitHub, and Netflix don't normalize everything. Instead:

```sql
CREATE TABLE entities (
  id UUID PRIMARY KEY,
  version INT NOT NULL,
  data JSONB NOT NULL
);
```

Your application knows how to read `version: 1`, `version: 2`, `version: 3`. No database migrations for new fields. Code handles backward compatibility.

This is an **architectural decision**, not laziness. It trades database complexity for application complexity—a worthwhile trade at scale.

**The problem isn't JSONB. The problem is claiming it's "temporary" when you're actually building an unversioned, undocumented schema-on-read system by accident.**

---
## Escaping the Trap

If you are already in this hole, stop digging.

1.  **Audit**: Run `jsonb_object_keys` to see what garbage you are actually storing.
2.  **Promote**: Identify the top 3 most used fields. Make them real columns.
3.  **Migrate**: Write a script to backfill the columns.
4.  **Drop**: Remove the keys from the JSONB blob.

Do not say "we will fix it later." **Fix it now.**

Later never comes. And your database is crying.
