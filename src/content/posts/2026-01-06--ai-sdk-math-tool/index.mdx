---
title: "Stop Asking LLMs to Do Math"
subTitle: "They are bad at it. Here is how to fix it."
date: 2026-01-06
modified: 2026-01-07
tags: [AI, AI SDK, TypeScript, Math, Tools, Patterns]
category: AI
subCategory: Engineering
cover_full_width: ./wide.webp
cover_mobile: ./square.webp
cover_icon: ./square.webp
---

I was reviewing a client's chatbot logs last week. It was painful.

They were using a state-of-the-art model to calculate potential mortgage payments. The model's confidence? 110%. The answer? Wrong by $400/month.

**This isn't a bug. It's a feature.**

LLMs are probabilistic token predictors, not calculators. They don't "compute" 2 + 2; they predict that "4" is the most likely token to follow. For creative writing, this fuzziness is magic. For financial calculations or engineering physics, **it is a critical risk.**

While newer models like GPT-5 claim better reasoning, they are still fundamentally guessing.

The solution isn't a bigger model. It's giving the model the tools to stop guessing.

Today, we're going to fix this using **LLM tool calling** (with [AI SDK v5](https://ai-sdk.vercel.ai/)) and a real symbolic math engine.

---

## The Stack

We need two things: something to manage the AI, and something that actually knows math (not just the *vibes* of math).

1.  **CortexJS Compute Engine**: A symbolic math engine that handles everything from basic arithmetic to calculus.
2.  **AI SDK v5**: The best way to build AI apps in TypeScript right now.

```bash
npm install ai @cortex-js/compute-engine zod
```

## The Implementation

The goal is simple: create a tool that the LLM **must** use when it detects a math problem. We aren't giving it `eval()` (please stop doing that). We are giving it a structured interface to a symbolic engine.

Here is the code. Copy this.

```typescript
import { generateText, tool } from 'ai';
import { ComputeEngine } from '@cortex-js/compute-engine';
import { z } from 'zod';

// Initialize the engine once
const ce = new ComputeEngine();

const mathTool = tool({
  description: 'Evaluate mathematical expressions and solve equations with guaranteed accuracy. MUST be used for all mathematical operations to verify correctness - do not attempt mental math. Supports arithmetic, algebra, calculus, and complex operations. Can process multiple expressions at once.',
  parameters: z.object({
    expressions: z.array(z.string()).describe(
      'Array of mathematical expressions in LaTeX or plain notation, e.g. ["2 + 2", "\\frac{x^2 + 1}{x - 1}", "\\int x^2 dx"]'
    ),
  }),
  execute: async ({ expressions }) => {
    // Process all expressions in parallel (or detailed batch)
    return expressions.map(expression => {
      try {
        const result = ce.parse(expression).evaluate();
        return {
          expression,
          result: result.toString(),
          latex: result.latex,
        };
      } catch (error) {
        return { 
          expression,
          error: (error as Error).message 
        };
      }
    });
  },
});
```

### Usage Example

Now, let's look at a real scenario. We'll ask a multi-step question that would typically cause a raw model to hallucinate digits.

```typescript
import { anthropic } from '@ai-sdk/anthropic';

const { text } = await generateText({
  model: anthropic('claude-4-5-sonnet-20251115'),
  prompt: 'Calculate 18472 Ã— 9347, divide by 127, then take the square root of the result.',
  tools: { math: mathTool },
  maxSteps: 5, // Allow the model to use the tool and then explain the result
});

console.log(text);
```

---

## Why This Implementation Works

There are a few patterns here that separate a "demo" from **production code**:

1.  **Mandatory Usage Instruction**: The description explicitly commands the model: "MUST be used... do not attempt mental math." Prompt engineering inside tool definitions is the only way to enforce reliable behavior.
2.  **Batch Processing**: The tool accepts an `expressions` array. **Latency kills user experience.** This allows the LLM to solve systems of equations or perform ten calculations in a single round-trip.
3.  **Symbolic Processing**: By using `@cortex-js/compute-engine`, we aren't just doing math; we are parsing intent. We can handle LaTeX inputs, integrals, and derivatives.
4.  **Structured Error Handling**: If one expression fails, the whole tool call doesn't crash. We return the error for that specific expression, allowing the agent to self-correct.

## Beyond Arithmetic

Since we are using a symbolic engine, we can handle queries that "calculator tools" often miss.

**Algebra:**
> "Solve these equations: 3x + 7 = 22 and 2y - 5 = 13"

**Calculus:**
> "Find the derivative of x^3 + 2x^2 and evaluate it at x = 2"

**Unit Conversion & LaTeX:**
The engine inherently understands LaTeX, making this perfect for educational apps where you need to render the output nicely on the frontend.

---

## Summary

Stop hoping your LLMs will "get better" at math. They are language models, not logic gates.

By binding a specialized math engine via AI SDK's tool system, you get the best of both worlds: the reasoning and natural language understanding of the LLM, and the rigorous precision of a computer algebra system.

Don't settle for "probably right."
